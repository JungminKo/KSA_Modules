{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. 크롤링과 스크레핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Crawling & Scraping\n",
    "- 근래에는 정형화된 데이터보다 비정형 자료들을 많이 분석함\n",
    "- Social Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빅데이터의 수집\n",
    ">### 빅데이터\n",
    "- 대규모의 데이터 셋 (정의의 일부)\n",
    "- 데이터 수집만으로 의미 X, 데이터를 활용했을 때 가치 부여\n",
    "- 기존의 Data warehouse는 정형화되어있는 자료들을 다룸 (ex) 표\n",
    "    - 빅데이터는 정형화 이외의 자료도 분석의 대상으로 봄\n",
    "- 빅데이터의 Definition은 가지가지\n",
    "    - IBM 3V\n",
    "        - 크기 (Value)\n",
    "        - 속도 (Velocity)\n",
    "        - 다양성 (Variety)\n",
    "- 실시간으로 생성되는 자료\n",
    "    - 블로그와 SNS를 이용한 트렌드 분석\n",
    "    - 인터넷 전자상거래 상품 DB 분석\n",
    "    - 금융 정보를 이용한 예측\n",
    "    - 공공데이터를 이용한 인구, 미세먼지 등 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### 스크레핑(Scaping)\n",
    "- _웹 사이트의 특정 정보를 추출하는 기술_\n",
    "- 공개된 정보는 대부분 HTML 형식으로 되어있어 필요한 데이터로 저장하기 위해 가공이 필요\n",
    "- 이를 위해 데이터 주고를 파악하는 것이 필요\n",
    "- 최근에는 로그인을 통해서만 유용한 정보에 접근할 수 있는 경우가 많다.\n",
    "    - 로그인 이후 필요한 웹 페이지 접근 기술이 필요\n",
    "- JS, 서버용 언어(PHP, ASP)가 중간에 끼기 때문에 접근이 어려울 수 있음\n",
    "\n",
    ">### 크롤링(Crawling)\n",
    "- _웹 사이트를 프로그램이 정기적으로 정보를 추출하는 기술_\n",
    "    - 크롤링하는 프로그램을 크롤러(crawler) 또는 스파이더(spider)\n",
    "\n",
    ">### 머신러닝에 사용할 수 있는 데이터 구조\n",
    "- 머신러닝뿐만 아니라 일반적인 분석에도 마찬가지, 수집한 자료를 편집하는 과정은 필수\n",
    "- 수집된 자료는 데이터의 구조를 분석하고 필요한 부분만 추출하여 분석 과정을 통해 머신러닝에 사용 가능\n",
    "- 데이터 형식은 파일 또는 DB에 저장하여 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Web component : HTML, HTTP\n",
    "\n",
    ">>### HTML\n",
    "- 프로그램 언어임 (물론 일반적인 언어와는 다름)\n",
    "- 태그(tag)는 꺾쇠 괄호 <>로 둘러싸여 있고, 그 안에 정보에 대한 의미를 작성\n",
    "- 그 의미가 끝나는 부분에 슬래시(/)를 사용하여 해당 태그를 종료\n",
    "        <title>Hello, World</title>       # 제목 요소, 값은 Hello, World\n",
    "        <img src=\"http://tcpschool.com/lectures/img_webbasic_10.png\" width=\"600\" height=\"300\"> # 맨 아래의 이미지 태그\n",
    "        \n",
    ">>### HTTP\n",
    "- HTTP(Hypertext Transaction Protocol)은 인터넷에서 컴퓨터 간에 정보를 주고받을 때 사용하는 일종의 약속\n",
    "- CS(Computer Science)에서 일반적으로 약속을 Protocol이라고 함\n",
    "\n",
    ">### 웹의 동작 순서\n",
    "- 웹 브라우저 실행 $\\rightarrow$ 주소 정보 입력\n",
    "- 주소 정보의 공식 이름은 **URL(Uniform Resource Locator)**\n",
    "- 출처 : https://joshua1988.github.io/web-development/http-part1/\n",
    "![title](https://joshua1988.github.io/images/posts/web/http/url-structure.png)\n",
    "- URL에는 해당 서버가 위치한 인터넷 주소 정보인 Domain Name이 존재\n",
    "- 흔히 도메인 정보 또는 서버 주소라고도 하는 이 주소를 통해 웹의 정보를 제공하는 서버에 접속\n",
    "- 일반적으로 컴퓨터는 **인터넷 프로토콜 주소(Internet Protocol Address)**, 즉 IP 주소라고 부르는 주소 값을 가짐\n",
    "    - IP 주소는 숫자로 되어있음. 이를 외우기는 쉽지 않음\n",
    "    - 보통 4가지 숫자를 이용하여 구성\n",
    "    - 즉, 외우기 쉽게 문자로 바꾼, domain name, URL을 사용함\n",
    "- IP 주소를 컴퓨터의 주소로 생각하면 이 주소에 접속하기 위해 사용하는 도메인 네임과 연결하기 위한 도메인 네임 서버(Domain Name Server, DNS)를 운영\n",
    "- 출처 : http://tcpschool.com/webbasic/works\n",
    "<img src=\"http://tcpschool.com/lectures/img_webbasic_10.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 다운로드 하기\n",
    ">### 웹상의 정보를 추출하는 방법\n",
    "- 웹 사이트의 텍스트를 가져오기 위해 프로그램적으로는 **Source**를 파악해야 한다.\n",
    "    - 즉, 최초에 source 분석을 해야한다.\n",
    "- 웹 사이트에 있는 데이터 추출을 위해 `urllib` 라이브러리 사용\n",
    "    - HTTP 또는 FTP(파일을 주고 받을 수 있는 protocol)를 이용해 데이터를 다운로드\n",
    "- `urllib`는 URL을 다루는 모듈을 모아놓은 패키지\n",
    "    - 특히 `urllib.request` 모듈은 웹 사이트에 있는 데이터에 접근하는 기능을 제공\n",
    "            urlertrieve(url, name)\n",
    "                - URL 주소의 파일을 다운로드\n",
    "            urlopen()\n",
    "                - 곧바로 파일을 저장하지 않고 메모리상에 load\n",
    "- 흔히 `a` 태그가 이동할 주소나 파일의 위치를 담고있음 (link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다운로드\n",
    "import urllib.request # 패키지 실행\n",
    "url = 'http://uta.pw/shodou/img/28/214.png' # 파일을 다운로드할 주소\n",
    "savename = 'test_download.png' # 저장할 파일 이름\n",
    "urllib.request.urlretrieve(url, savename) # 파일 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('test_download.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 상에 주소를 저장\n",
    "# 메모리 상에 load 후 바이너리 파일로 변환하여 파일을 저장\n",
    "url = 'http://uta.pw/shodou/img/28/214.png' # 파일을 다운로드할 주소\n",
    "savename = 'open_download.png' # 저장할 파일 이름\n",
    "# 파일 다운로드\n",
    "memory = urllib.request.urlopen(url).read() # URL 리소스를 열로 read 메소드 데이터 읽기\n",
    "# 파일로 저장\n",
    "with open(savename, 'wb') as f: # w 쓰기 모드, b 바이너리 모드\n",
    "    f.write(memory) # 메소드로 다운로드한 바이너리 데이터를 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공공데이터 포털\n",
    "url = 'https://www.data.go.kr/dataset/fileDownload.do?atchFileId=FILE_000000001455071&fileDetailSn=1&publicDataDetailPk=uddi:baa36625-7a28-4d54-b118-dc47ba14378c'\n",
    "savename = 'gas_20180620.csv'\n",
    "urllib.request.urlretrieve(url, savename) # 파일 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    절대주소 : http://www.exam.com/s/Ex.txt\n",
    "    상대주소 : s/Ex.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('공공데이터_html_예시.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = pd.read_csv(savename, encoding='cp949')\n",
    "gas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### 웹에서 데이터 추출하기\n",
    "- 웹에서 XML 또는 HTML 등의 텍스트 기반 데이터를 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어 들이기\n",
    "url = 'http://api.aoikujira.com/ip/ini' # 데이터를 가져올 주소\n",
    "res = urllib.request.urlopen(url) # URL 리소스를 열기\n",
    "data = res.read() # 바이너리 데이터로 읽기\n",
    "print(data) # 바이너리로 출력\n",
    "\n",
    "# 바이너리를 문자열로 변환 (HTML 소스 불러오기)\n",
    "text = data.decode('utf-8') # decode aptjemfmf \n",
    "print('\\n', text) # 문자열로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어 들이기\n",
    "url = 'http://fun-coding.org/crawl_basic2.html' # 데이터를 가져올 주소\n",
    "res = urllib.request.urlopen(url) # URL 리소스를 열기\n",
    "data = res.read() # 바이너리 데이터로 읽기\n",
    "\n",
    "# 바이너리를 문자열로 변환 (HTML 소스 불러오기)\n",
    "text = data.decode('utf-8') # decode aptjemfmf \n",
    "print(text.split('<body')[0]) # 문자열로 출력\n",
    "                              # Head만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### 매개변수를 추가해 요청을 전송하는 방법\n",
    "- URL에 매개변수를 추가해 요청을 전손\n",
    "- 기상청 RSS 서비스\n",
    "    - http://www.weather.go.kr/weather/lifenindustry/service_rss.jsp\n",
    "    - http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnld=108\n",
    "         - ? 다음의 stnld=108가 매개변수\n",
    "- URL 끝부분에 ?를 입력하고 \\<key\\>=\\<value\\> 형식으로 매개변수 입력\n",
    "- 여러 개의 매개 변수인 경우, &를 사용하여 구분\n",
    "    - https://www.data.go.kr/search/index.do?\n",
    "          query : (index=OPENAPI) & (query=%EA%B4%80%EA%B4%91) & (currentPage=3) & (countPerPage=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`urllib.parse.urlencode()`\n",
    "- 굉장히 중요\n",
    "- 한글을 parsing해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request # 패키지 실행\n",
    "import urllib.parse # 패키지 실행\n",
    "# 데이터 읽기\n",
    "API = 'http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp' # 매개변수를 URL 인코딩\n",
    "# 매개변수(딕셔너리 자료형)을 URL 인코딩\n",
    "value = {'stnld' : '108'}\n",
    "params = urllib.parse.urlencode(value) # 매개변수를 URL 인코딩\n",
    "print(params)\n",
    "\n",
    "# 요청 URL 생성\n",
    "url = API + '?' + params # URL 리소스 열기\n",
    "print('url={0}'.format(url))\n",
    "\n",
    "# 다운로드\n",
    "res = urllib.request.urlopen(url) # URL 리소스 열기\n",
    "data = res.read() # 바이너리 데이터로 읽기\n",
    "\n",
    "# 바이너리를 문자열로 변환 (HTML 소스 불러오기)\n",
    "text = data.decode('utf-8')\n",
    "print(text.split('<body')[0]) # 문자열로 출력\n",
    "                              # Header만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### 매개변수를 명령줄에서 지정하기\n",
    "- 앞선 프로그램에서는 매개변수를 코드에서 입력해야 하므로 다른 지역은 매개변수를 지정하려면 프로그램을 수정\n",
    "- 명령줄에서 바로 지역번호를 입력하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 호출\n",
    "import sys\n",
    "import urllib.request as req\n",
    "import urllib.parse as parse\n",
    "\n",
    "# 입력줄 매개변수 추출\n",
    "text = [] # 문서를 저장할 리스트 초기화\n",
    "while True:\n",
    "    # 입력줄을 이용하여 지역번호 입력\n",
    "    region_number = input('Usage : download-forecast-argv : ')\n",
    "    \n",
    "    # 반복구문 졸료 조건\n",
    "    if region_number.upper() == 'EXIT':\n",
    "        break\n",
    "    elif int(region_number) not in [108, 109, 105, 131, 133, 146, 156, 134, 159, 184]:\n",
    "        continue\n",
    "    \n",
    "    # 매개변수를 URL 인코딩(한글을 포함하는 경우 필수로 실행)\n",
    "    API = 'http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp'\n",
    "    \n",
    "    values = {'stnld' : region_number}\n",
    "    params = parse.urlencode(values)\n",
    "    url = API + '?' + params\n",
    "    print('URL=', url)\n",
    "    \n",
    "    # 페이지 다운로드\n",
    "    data = req.urlopen(url).read()\n",
    "    text.append(data.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup으로 scraping하기\n",
    ">### BeautifulSoup으로 scraping\n",
    "- 스크레핑이란 웹 사이트에서 데이터를 추출하여 원하는 정보를 얻어내는 것\n",
    "- 파이썬에서 스크레핑할 때 빼놓을 수 없는 라이브러리, BeautifulSoup\n",
    "        pip install beautifulsoup4\n",
    "    - BeautifulSoup을 HTML과 XML에서 정보 추출이 가능\n",
    "    - HTML과 XML 분석을 해주는 라이브러리\n",
    "    - 자체로 다운로드 기능은 없음\n",
    "- HTML 구조로 요소를 추출 `BeautifulSoup()` 함수 사용\n",
    "    - HTML 구조로 요소를 추출하는 것, HTML 구조를 하나하나 적어나가는 것은 매우 복잡\n",
    "    - 간단하게 요소를 찾아내는 방법이 필요\n",
    "        <table style=\"width:600px\">\n",
    "          <thead>\n",
    "            <tr>\n",
    "              <th style=\"width:200px\">markup parser</th>\n",
    "              <th style=\"width:400px\">설명</th>\n",
    "            </tr>\n",
    "          </thead>\n",
    "          <tbody>\n",
    "            <tr>\n",
    "              <td><code class=\"highlighter-rouge\">html.parser</code></td>\n",
    "              <td>기본옵션으로 빠르지만 유연하지 못함 (단순하 html 문서에서 사용)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "              <td><code class=\"highlighter-rouge\">lxml</code></td>\n",
    "              <td>매우 빠르고 유연</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "              <td><code class=\"highlighter-rouge\">xml</code></td>\n",
    "              <td>XML 파일에만 사용</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "              <td><code class=\"highlighter-rouge\">html5lib</code></td>\n",
    "              <td>매우 느리지만 유연 (구조가 복잡한 HTML 문서에 사용)</td>\n",
    "            </tr>\n",
    "          </tbody>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 호출\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 분석할 HTML\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "<h1>스크레핑이란?</h1>\n",
    "<p>웹 페이지를 분석하는 것</p>\n",
    "<p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(html, 'html.parser') # BeautifulSoup 객체 생성\n",
    "print(soup, '\\n')\n",
    "print(soup.prettify(), '\\n')\n",
    "\n",
    "# 원하는 부분 추출\n",
    "h1 = soup.html.body.h1 # HTML 태그 구조를 이용하여 접근\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling # nest_sibling 첫 번째 p태그 다음 공백 줄바꿈 문자\n",
    "                                  # next_sibling 두 번째 p태그\n",
    "print(p1) # p1 tag\n",
    "print(p1.next_sibling) # \\n\n",
    "print(p1.next_sibling.next_sibling) # p2 tag\n",
    "\n",
    "# 요소의 글자 출력\n",
    "print('h1 = {0}'.format(h1.string))\n",
    "print('p1 = {0}'.format(p1.string))\n",
    "print('p2 = {0}'.format(p2.string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = soup.body.p\n",
    "p1, P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.p.previous_sibling # 줄바꿈 문자를 읽어와보리기~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### id로 요소를 찾는 방법\n",
    "- id 속성을 지정하여 요소를 찾는 `find()` 메서드를 제공\n",
    "        find(tag_name, attrs={}) 메서드를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HTML_Exam.html\n",
    "```\n",
    "<html>\n",
    "\t<head>\n",
    "\t\t<title>Page title</title>\n",
    "\t</head>\n",
    "\t<body>\n",
    "    \t<div>\n",
    "            <p>a</p>\n",
    "            <p>b</p>\n",
    "            <p>c</p>\n",
    "        </div>\n",
    "        <div class=\"ex_class\">\n",
    "            <p>d</p>\n",
    "            <p>e</p>\n",
    "            <p>f</p>\n",
    "        </div>\n",
    "        <div id=\"ex_id\">\n",
    "            <p>g</p>\n",
    "            <p>h</p>\n",
    "            <p>i</p>\n",
    "        </div>\n",
    "\t\t<h1>This is a heading</h1>\n",
    "\t\t<p>This is a paragraph.</p>\n",
    "\t\t<p>This is another paragraph.</p>\n",
    "\t</body>\n",
    "</html>\n",
    "```\n",
    "<html>\n",
    "\t<head>\n",
    "\t\t<title>Page title</title>\n",
    "\t</head>\n",
    "\t<body>\n",
    "    \t<div>\n",
    "            <p>a</p>\n",
    "            <p>b</p>\n",
    "            <p>c</p>\n",
    "        </div>\n",
    "        <div class=\"ex_class\">\n",
    "            <p>d</p>\n",
    "            <p>e</p>\n",
    "            <p>f</p>\n",
    "        </div>\n",
    "        <div id=\"ex_id\">\n",
    "            <p>g</p>\n",
    "            <p>h</p>\n",
    "            <p>i</p>\n",
    "        </div>\n",
    "\t\t<h1>This is a heading</h1>\n",
    "\t\t<p>This is a paragraph.</p>\n",
    "\t\t<p>This is another paragraph.</p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 열기\n",
    "fp = open('C:/workspace/KSA\\data/modeule04/ch01/HTML_Exam.html', 'r',\n",
    "         encoding='utf-8')\n",
    "\n",
    "soup = BeautifulSoup(fp, 'html.parser') # BeautifulSoup 객체 생성\n",
    "divs = soup.find('div') # div 태그\n",
    "divs_class = soup.find('div', class_='ex_class') # div 태그, class\n",
    "divs_id = soup.find('div', id='ex_id') # div 태그, id\n",
    "p = soup.find('p') # p 태그\n",
    "print('*** div 태그\\n{0}'.format(divs), '\\n')\n",
    "print('*** div 태그, class=ex_class\\n{0}'.format(divs_class), '\\n')\n",
    "print('*** div 태그, id=ex_id\\n{0}'.format(divs_id), '\\n')\n",
    "print('*** p 태그\\n{0}'.format(p))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs.text, divs.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.next_sibling.next_sibling.next_sibling.next_sibling # div 태그 이하로는 내려가지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.div.next_sibling.next_sibling.next_sibling.next_sibling.p.next_sibling.next_sibling # 너무 복잡한디...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 여러 개의 요소 추출하기\n",
    "        find_all(tag_name, attrs={}) 메서드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 열기\n",
    "fp = open('C:/workspace/KSA\\data/modeule04/ch01/HTML_Exam.html', 'r',\n",
    "         encoding='utf-8')\n",
    "\n",
    "soup = BeautifulSoup(fp, 'html.parser') # BeautifulSoup 객체 생성\n",
    "divs = soup.find_all('div') # 모든 div 태그\n",
    "print('*** div 태그\\n{0}'.format(divs))\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">- 온라인 파일 열기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# 온라인 파일 열기\n",
    "url = urlopen('http://www.naver.com')\n",
    "soup = BeautifulSoup(url, 'html.parser')\n",
    "a = soup.find_all('a', class_='ah_da') # 모든 a 태그, class='ah_da'\n",
    "print('*** a 태그, class=\"ah_da\"\\n{0}'.format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag = soup.find_all('div', class_='api_atcmp_wrap _keywords', style=\"display:none;\")\n",
    "tag = soup.find_all('div', {'class':'api_atcmp_wrap _keywords', 'style':\"display:none;\"}) # dict로 넣을 땐 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 온라인 파일 열기\n",
    "url = urlopen('http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp')\n",
    "soup = BeautifulSoup(url, 'html.parser')\n",
    "title = soup.find('title').string # title tag\n",
    "wf = soup.find('wf') # wf 태그\n",
    "print('Title\\n{0}'.format(title))\n",
    "print('wf{0}'.format(wf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 온라인 파일 열기\n",
    "url = urlopen('http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp')\n",
    "soup = BeautifulSoup(url, 'html.parser')\n",
    "title = soup.find('title').string # title tag\n",
    "wf = soup.find_all('wf') # 모든 wf 태그\n",
    "print('Title\\n{0}'.format(title))\n",
    "for ix, x in enumerate(wf):\n",
    "    if ix > 40:\n",
    "        break\n",
    "    text = x.string # wf tag 하나의 문자열\n",
    "    print('wf : {0}'.format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
