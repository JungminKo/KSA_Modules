{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어제 코드 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "%pylab inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================#\n",
    "## First-visit Monte Carlo\n",
    "#===============================#\n",
    "\n",
    "# parameters\n",
    "gamma = 0.6 # discounting rate\n",
    "rewardSize = -1\n",
    "gridSize = 4\n",
    "terminationStates = [[0,0], [gridSize-1, gridSize-1]]\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]  # up, down, left, right\n",
    "numIterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "V = np.zeros((gridSize, gridSize))\n",
    "returns = {(i, j):list() for i in range(gridSize) for j in range(gridSize)} #dictionary with keys and values \n",
    "deltas = {(i, j):list() for i in range(gridSize) for j in range(gridSize)}  #dictionary with keys and values \n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): [],\n",
       " (0, 1): [],\n",
       " (0, 2): [],\n",
       " (0, 3): [],\n",
       " (1, 0): [],\n",
       " (1, 1): [],\n",
       " (1, 2): [],\n",
       " (1, 3): [],\n",
       " (2, 0): [],\n",
       " (2, 1): [],\n",
       " (2, 2): [],\n",
       " (2, 3): [],\n",
       " (3, 0): [],\n",
       " (3, 1): [],\n",
       " (3, 2): [],\n",
       " (3, 3): []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns == deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " [0, 1],\n",
       " [0, 2],\n",
       " [0, 3],\n",
       " [1, 0],\n",
       " [1, 1],\n",
       " [1, 2],\n",
       " [1, 3],\n",
       " [2, 0],\n",
       " [2, 1],\n",
       " [2, 2],\n",
       " [2, 3],\n",
       " [3, 0],\n",
       " [3, 1],\n",
       " [3, 2],\n",
       " [3, 3]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def generateEpisode():\n",
    "    initState = random.choice(states[1:-1])\n",
    "    episode = []\n",
    "    while True:\n",
    "        if list(initState) in terminationStates:\n",
    "            return episode\n",
    "        action = random.choice(actions)\n",
    "        finalState = np.array(initState)+np.array(action)\n",
    "        if -1 in list(finalState) or gridSize in list(finalState):\n",
    "            finalState = initState\n",
    "        episode.append([list(initState), action, rewardSize, list(finalState)])\n",
    "        initState = finalState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[3, 1], [-1, 0], -1, [2, 1]],\n",
       " [[2, 1], [0, -1], -1, [2, 0]],\n",
       " [[2, 0], [0, 1], -1, [2, 1]],\n",
       " [[2, 1], [0, 1], -1, [2, 2]],\n",
       " [[2, 2], [0, 1], -1, [2, 3]],\n",
       " [[2, 3], [-1, 0], -1, [1, 3]],\n",
       " [[1, 3], [0, 1], -1, [1, 3]],\n",
       " [[1, 3], [0, 1], -1, [1, 3]],\n",
       " [[1, 3], [0, -1], -1, [1, 2]],\n",
       " [[1, 2], [1, 0], -1, [2, 2]],\n",
       " [[2, 2], [0, -1], -1, [2, 1]],\n",
       " [[2, 1], [0, 1], -1, [2, 2]],\n",
       " [[2, 2], [1, 0], -1, [3, 2]],\n",
       " [[3, 2], [1, 0], -1, [3, 2]],\n",
       " [[3, 2], [0, 1], -1, [3, 3]]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode = generateEpisode()\n",
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[3, 2], [0, 1], -1, [3, 3]]\n",
      "1 [[3, 2], [1, 0], -1, [3, 2]]\n",
      "2 [[2, 2], [1, 0], -1, [3, 2]]\n",
      "3 [[2, 1], [0, 1], -1, [2, 2]]\n",
      "4 [[2, 2], [0, -1], -1, [2, 1]]\n",
      "5 [[1, 2], [1, 0], -1, [2, 2]]\n",
      "6 [[1, 3], [0, -1], -1, [1, 2]]\n",
      "7 [[1, 3], [0, 1], -1, [1, 3]]\n",
      "8 [[1, 3], [0, 1], -1, [1, 3]]\n",
      "9 [[2, 3], [-1, 0], -1, [1, 3]]\n",
      "10 [[2, 2], [0, 1], -1, [2, 3]]\n",
      "11 [[2, 1], [0, 1], -1, [2, 2]]\n",
      "12 [[2, 0], [0, 1], -1, [2, 1]]\n",
      "13 [[2, 1], [0, -1], -1, [2, 0]]\n",
      "14 [[3, 1], [-1, 0], -1, [2, 1]]\n"
     ]
    }
   ],
   "source": [
    "for i, step in enumerate(episode[::-1]):\n",
    "    print(i, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "gamma = 0.6 # discounting rate\n",
    "rewardSize = -1\n",
    "gridSize = 4\n",
    "terminationStates = [[0,0], [gridSize-1, gridSize-1]]\n",
    "\n",
    "i = 0\n",
    "step = [[3, 2], [0, 1], -1, [3, 3]]\n",
    "\n",
    "G = gamma * G + step[2]\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root : []\n",
      "these step : [3, 2]\n",
      "step[0] not in root ? : True\n"
     ]
    }
   ],
   "source": [
    "print('root :', [x[0] for x in episode[::-1][len(episode)-i:]])\n",
    "print('these step :', step[0])\n",
    "print('step[0] not in root ? :', \n",
    "      step[0] not in [x[0] for x in episode[::-1][len(episode)-i:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "idx = tuple(step[0])\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2) : [-1.0]\n"
     ]
    }
   ],
   "source": [
    "returns[idx].append(G)\n",
    "print(idx, \":\", returns[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TalkOn Sermina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class GridWorld():\n",
    "    def __init__(self, init_pt=(0,0), size=4, end_pts=[(3,3)]):\n",
    "        self.x=init_pt[0]\n",
    "        self.y=init_pt[1]\n",
    "        self.size=size\n",
    "        assert len(end_pts) > 0, 'end_pt is required!!'\n",
    "        self.end_pts = end_pts\n",
    "        self.history = []\n",
    "    \n",
    "    def move_right(self):\n",
    "        self.y += 1  \n",
    "        if self.y > self.size - 1:\n",
    "            self.y = self.size - 1\n",
    "\n",
    "    def move_left(self):\n",
    "        self.y -= 1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "\n",
    "    def move_up(self):\n",
    "        self.x -= 1\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "\n",
    "    def move_down(self):\n",
    "        self.x += 1\n",
    "        if self.x > self.size - 1:\n",
    "            self.x = self.size - 1\n",
    "\n",
    "    def move_random(self):\n",
    "        coin = random.randint(0, self.size - 1)\n",
    "        if coin==0:\n",
    "            self.move_right()\n",
    "        elif coin==1:\n",
    "            self.move_left()\n",
    "        elif coin==2:\n",
    "            self.move_up()\n",
    "        else:\n",
    "            self.move_down()\n",
    "\n",
    "        self.history.append((self.x, self.y))\n",
    "\n",
    "    def move_random_until_end(self):\n",
    "        while True:\n",
    "            self.move_random()\n",
    "\n",
    "            if (self.x, self.y) in self.end_pts:\n",
    "                history = self.history\n",
    "                self.initialize()\n",
    "                return history\n",
    "\n",
    "    def initialize(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MonteCarlo\n",
    "data = np.zeros((4,4))\n",
    "env = GridWorld(init_pt=, )\n",
    "\n",
    "for k in range(10000):\n",
    "    history = env.move_random_until_end()\n",
    "    cum_reward = 0\n",
    "    \n",
    "    for position in history[::-1]:\n",
    "        x, y = position\n",
    "        data[x, y] = 0.999*data[x][y] + 0.001*cum_reward\n",
    "        cum_reward -= 1\n",
    "        \n",
    "for row in data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================#\n",
    "## First-visit Monte Carlo\n",
    "#===============================#\n",
    "\n",
    "# parameters\n",
    "gamma = 0.6 # discounting rate\n",
    "rewardSize = -1\n",
    "gridSize = 4\n",
    "terminationStates = [[0,0], [gridSize-1, gridSize-1]]\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]  # up, down, left, right\n",
    "numIterations = 10000\n",
    "\n",
    "# initialization\n",
    "V = np.zeros((gridSize, gridSize))\n",
    "returns = {(i, j):list() for i in range(gridSize) for j in range(gridSize)} #dictionary with keys and values \n",
    "deltas = {(i, j):list() for i in range(gridSize) for j in range(gridSize)}  #dictionary with keys and values \n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "\n",
    "# utils\n",
    "def generateEpisode():\n",
    "    initState = random.choice(states[1:-1])\n",
    "    episode = []\n",
    "    while True:\n",
    "        if list(initState) in terminationStates:\n",
    "            return episode\n",
    "        action = random.choice(actions)\n",
    "        finalState = np.array(initState)+np.array(action)\n",
    "        if -1 in list(finalState) or gridSize in list(finalState):\n",
    "            finalState = initState\n",
    "        episode.append([list(initState), action, rewardSize, list(finalState)])\n",
    "        initState = finalState\n",
    "\n",
    "for it in range(numIterations):  #상태바 출력\n",
    "    episode = generateEpisode()\n",
    "    G = 0\n",
    "    for i, step in enumerate(episode[::-1]):\n",
    "        G = gamma*G + step[2]\n",
    "        if step[0] not in [x[0] for x in episode[::-1][len(episode)-i:]]:\n",
    "            idx = (step[0][0], step[0][1])\n",
    "            returns[idx].append(G)\n",
    "            newValue = np.average(returns[idx])\n",
    "            deltas[idx[0], idx[1]].append(np.abs(V[idx[0], idx[1]]-newValue))\n",
    "            V[idx[0], idx[1]] = newValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.64232621, -2.21195304, -2.36896102],\n",
       "       [-1.63132908, -2.11110327, -2.28107273, -2.20660007],\n",
       "       [-2.21156773, -2.28402607, -2.10477695, -1.63555545],\n",
       "       [-2.37256685, -2.21078312, -1.62941446,  0.        ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "#====== 환경 로딩 =======#\n",
    "env = gym.make('Taxi-v2')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====== 신경망 구현 =======#\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 행동을 선택하는데 사용되는 신경망의 피드-포워드 부분을 구축한다.\n",
    "inputs = tf.placeholder(shape=[1, env.observation_space.n], \n",
    "                        dtype=tf.float32)  # 1*500 matrix\n",
    "weights = tf.Variable(tf.random_uniform(\n",
    "    [env.observation_space.n,env.action_space.n], 0, 0.01))  # 500*6 matrix\n",
    "q_out = tf.matmul(inputs, weights)  # 1*6 matrix\n",
    "predict = tf.argmax(q_out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표 Q값(ext_q)과 예측 Q값(q_out)의 제곱합을 구함으로써 비용을 얻게 된다.\n",
    "next_q = tf.placeholder(shape=[1,env.action_space.n], # 1*500\n",
    "                        dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(next_q - q_out))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "loss_update = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(500)[state:state+1] # 임의의 한 점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9838., 10057., 10017.,  9949., 10089.,  9998., 10110.,  9904.,\n",
       "         9950., 10088.]),\n",
       " array([1.30747080e-05, 1.00011278e-01, 2.00009481e-01, 3.00007685e-01,\n",
       "        4.00005888e-01, 5.00004091e-01, 6.00002295e-01, 7.00000498e-01,\n",
       "        7.99998701e-01, 8.99996905e-01, 9.99995108e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.random.rand(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1715.,    0., 1724.,    0., 1681.,    0., 1646.,    0., 1609.,\n",
       "        1625.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(10000):\n",
    "    res.append(env.action_space.sample())\n",
    "plt.hist(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====== 신경망 학습하기 =======#\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 학습 관련 파라미터를 설정한다.\n",
    "gamma = 0.7\n",
    "epsilon = 0.2\n",
    "epsilon_decay = .99\n",
    "episodes = 100\n",
    "\n",
    "total_epochs = 0\n",
    "total_rewards = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for episode in range(episodes):\n",
    "        #환경을 리셋하고 첫번째 새로운 관측값을 얻는다.\n",
    "        state = env.reset()\n",
    "        rewards_this_episode = 0\n",
    "        epochs = 0\n",
    "\n",
    "        done = False\n",
    "        \n",
    "        # q-network \n",
    "        while not done:\n",
    "            #Q-네트워크로부터 (e의 확률로 랜덤한 액션과 함께) 그리디하게 액션을 선택한다.\n",
    "            action, q = sess.run(\n",
    "                [predict,q_out], \n",
    "                feed_dict={\n",
    "                    inputs:np.identity(env.observation_space.n)[state:state + 1]\n",
    "                })\n",
    "            if np.random.rand(1) < epsilon: # 균등분포\n",
    "                action[0] = env.action_space.sample()\n",
    "\n",
    "            #환경으로부터 새로운 상태와 보상을 얻는다.                \n",
    "            next_state, reward, done, info = env.step(action[0])\n",
    "            #새로운 상태를 네트워크에 피드해줌으로써 Q’값을 구한다.\n",
    "            curr_q = sess.run(q_out, feed_dict = {inputs:np.identity(env.observation_space.n)[next_state:next_state+1]})\n",
    "            #maxQ'값을 구하고 선택된 행동에 대한 타겟 값을 설정한다.\n",
    "            max_next_q = np.max(curr_q)\n",
    "            target_q = q\n",
    "            target_q[0, action[0]] = reward + gamma * max_next_q\n",
    "\n",
    "            #타겟과 예측된 Q값을 이용하여 네트워크를 학습시킨다.\n",
    "            info, new_weights = sess.run([loss_update, weights], feed_dict={inputs:np.identity(env.observation_space.n)[state:state+1], next_q:target_q})\n",
    "            rewards_this_episode += reward\n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "        #모델을 학습해 나감에 따라 랜덤 액션의 가능성을 줄여간다.    \n",
    "        epsilon = epsilon * epsilon_decay\n",
    "        \n",
    "        total_epochs += epochs\n",
    "        total_rewards += rewards_this_episode\n",
    "        \n",
    "print (\"Percent of succesful episodes: \" + str(total_rewards/episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gridworld\n",
    "https://github.com/awjuliani/DeepRL-Agents/blob/master/gridworld.py\n",
    "@author: Administrator\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gameOb(): # Game Object\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gameEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:94: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:95: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:96: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "partial = False\n",
    "size = 5\n",
    "env = gameEnv(partial=partial, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hero (0, 0)\n",
      "goal (4, 1)\n",
      "fire (4, 4)\n",
      "goal (3, 4)\n",
      "fire (0, 1)\n",
      "goal (3, 3)\n",
      "goal (2, 0)\n"
     ]
    }
   ],
   "source": [
    "for ob in env.objects:\n",
    "    print(ob.name, (ob.x, ob.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "(0, 4)\n",
      "(1, 0)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "(1, 4)\n",
      "(2, 0)\n",
      "(2, 1)\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "(2, 4)\n",
      "(3, 0)\n",
      "(3, 1)\n",
      "(3, 2)\n",
      "(3, 3)\n",
      "(3, 4)\n",
      "(4, 0)\n",
      "(4, 1)\n",
      "(4, 2)\n",
      "(4, 3)\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "iterables = [range(5), range(5)]\n",
    "for t in itertools.product(*iterables):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]]), (7, 7, 3))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones([5+2,\n",
    "             5+2,\n",
    "             3])\n",
    "a[1:-1,1:-1,:] = 0\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hero \n",
      "\n",
      "\n",
      "goal \n",
      "\n",
      "\n",
      "fire \n",
      "\n",
      "\n",
      "goal \n",
      "\n",
      "\n",
      "fire \n",
      "\n",
      "\n",
      "goal \n",
      "\n",
      "\n",
      "goal \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in env.objects:\n",
    "    print(item.name, '\\n\\n')\n",
    "    a[item.y+1:item.y+item.size+1,\n",
    "      item.x+1:item.x+item.size+1,\n",
    "      item.channel] = item.intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(a[:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255 255 255 255 255 255 255 255 255 255]\n",
      " [255   0   0   0   0   0   0   0   0 255]\n",
      " [255   0   0   0   0   0   0   0   0 255]\n",
      " [255 255 255   0   0   0   0   0   0 255]\n",
      " [255   0   0   0   0   0   0   0   0 255]\n",
      " [255   0   0   0   0   0   0   0   0 255]\n",
      " [255   0   0   0   0   0   0   0   0 255]\n",
      " [255   0   0   0   0   0   0 255 255 255]\n",
      " [255   0   0   0   0   0   0 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[255., 255., 255., 255., 255., 255., 255., 255., 255., 255.],\n",
       "       [255.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 255.],\n",
       "       [255.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 255.],\n",
       "       [255., 255., 255.,   0.,   0.,   0.,   0.,   0.,   0., 255.],\n",
       "       [255.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 255.],\n",
       "       [255.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 255.],\n",
       "       [255.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 255.],\n",
       "       [255.,   0.,   0.,   0.,   0.,   0.,   0., 255., 255., 255.],\n",
       "       [255.,   0.,   0.,   0.,   0.,   0.,   0., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "print(scipy.misc.imresize(a[:,:,0], [10,10,1], interp='nearest'))\n",
    "np.array(Image.fromarray(a[:,:,0], mode=None).resize((10, 10))) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(Image.fromarray(a[:,:,0], mode=None).resize((84, 84))) * 255\n",
    "c = np.array(Image.fromarray(a[:,:,1], mode=None).resize((84, 84))) * 255\n",
    "d = np.array(Image.fromarray(a[:,:,2], mode=None).resize((84, 84))) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 3)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([b, c, d], axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gameEnv():\n",
    "    def __init__(self,partial,size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = [] # object를 저장하는 리스트\n",
    "        self.partial = partial\n",
    "        # self.renderEnv()에서 반환된 RGB 이미지를 반환\n",
    "        a = self.reset()\n",
    "        plt.imshow(a,interpolation=\"nearest\")\n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        \"\"\"\n",
    "        -----gameob parameter----\n",
    "        coordinates\n",
    "        size = 1\n",
    "        intensity = 1\n",
    "        channel = 0, 1, 2; hero, hole, bug 구분 채널\n",
    "        reward = None. 1, -1; hole에 가면 -1, bug에 가면 +1\n",
    "        name = 'hero', 'goal', 'fire'\n",
    "        \"\"\"\n",
    "        # self.newPosition() : \n",
    "        # 1. 좌표평면 상에 다른 object가 있는지 탐색\n",
    "        # 2. 존재한다면 그 위치를 제외한 임의의 위치에 obejct를 위치시킴\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        bug = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug)\n",
    "        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole)\n",
    "        bug2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug2)\n",
    "        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole2)\n",
    "        bug3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug3)\n",
    "        bug4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug4)\n",
    "        # self.renderEnv() :\n",
    "        # 각각 hero, goal, fire는 다른 channel값을 가진다.\n",
    "        # 3개 >> RGB로 표현할 계획\n",
    "        # 이를 channel별로 (7,7,3) 배열로 뿌려주고\n",
    "        # a[:,:,channel]을 (84,84)로 resize한 후에\n",
    "        # 이를 합친 배열을 return한다.\n",
    "        # 만일 partial=True 옵션을 부여하면, hero를 기준으로 3*3을 84*84로 반환\n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        return state\n",
    "\n",
    "    def moveChar(self,direction):\n",
    "        # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        penalize = 0.\n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX-2:\n",
    "            hero.x += 1     \n",
    "        if hero.x == heroX and hero.y == heroY:\n",
    "            penalize = 0.0\n",
    "        self.objects[0] = hero\n",
    "        return penalize\n",
    "    \n",
    "    def newPosition(self):\n",
    "        # 좌표 생성 ---------------------------------------------------\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY) ]\n",
    "        points = [] # size * size의 total 좌표 저장\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        # object 현재 위치 탐색 ----------------------------------------\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x, objectA.y) not in currentPositions: # object의 현재 위치 지정\n",
    "                currentPositions.append(\n",
    "                    (objectA.x,objectA.y)\n",
    "                ) \n",
    "        # 좌표평면 상에서 object가 위치하고 있는 좌표 제거 -------------\n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        # randomize하게 이미 존재하는 object가 위치하는 자리를 제외한\n",
    "        # position을 반환 ----------------------------------------------\n",
    "        location = np.random.choice(range(len(points)),replace=False)\n",
    "        return points[location]\n",
    "\n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        ended = False\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n",
    "                else: \n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n",
    "                return other.reward,False\n",
    "        if ended == False:\n",
    "            return 0.0,False\n",
    "\n",
    "    def renderEnv(self):\n",
    "        #a = np.zeros([self.sizeY,self.sizeX,3])\n",
    "        # (7,7,3)으로 render, channel수는 3\n",
    "        # 7,7을 channel로 뿌려줌 --------------------------\n",
    "        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        a[1:-1,1:-1,:] = 0\n",
    "        hero = None\n",
    "        for item in self.objects:\n",
    "            a[item.y+1:item.y+item.size+1,\n",
    "              item.x+1:item.x+item.size+1,\n",
    "              item.channel] = item.intensity\n",
    "            if item.name == 'hero':\n",
    "                hero = item\n",
    "        if self.partial == True:\n",
    "            a = a[hero.y:hero.y+3,hero.x:hero.x+3,:]\n",
    "        # Depreciation scipy 1.3.0\n",
    "#         b = scipy.misc.imresize(a[:,:,0],[84,84,1],interp='nearest')\n",
    "#         c = scipy.misc.imresize(a[:,:,1],[84,84,1],interp='nearest')\n",
    "#         d = scipy.misc.imresize(a[:,:,2],[84,84,1],interp='nearest')\n",
    "        b = np.array(Image.fromarray(a[:,:,0], mode=None).resize((84, 84))) * 255\n",
    "        c = np.array(Image.fromarray(a[:,:,1], mode=None).resize((84, 84))) * 255\n",
    "        d = np.array(Image.fromarray(a[:,:,2], mode=None).resize((84, 84))) * 255\n",
    "        # hero, goal, fire를 RGB 새게 채널로 만들어 합쳐서 리던 -------\n",
    "        a = np.stack([b,c,d],axis=2)\n",
    "        return a\n",
    "\n",
    "    def step(self,action):\n",
    "        penalty = self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        if reward == None:\n",
    "            print(done)\n",
    "            print(reward)\n",
    "            print(penalty)\n",
    "            return state,(reward+penalty),done\n",
    "        else:\n",
    "            return state,(reward+penalty),done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Double Dueling DQN: Gridworld\n",
    "http://localhost:8888/notebooks/Desktop/%EA%B0%95%EC%9D%98%EA%B4%80%EB%A0%A8/Chap6.Double-Dueling-DQN.ipynb\n",
    "강화학습 첫걸음\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "에이전트는 파란색 사각형을 위, 아래, 왼쪽, 오른쪽으로 이동시킨다. \n",
    "목표는 빨간색 사각형 (-1의 보상)을 피하여 녹색 사각형 (+1의 보상)까지 도달하는 것이다. \n",
    "세가지 블록의 위치는 매 에피소드마다 랜덤하게 변하게 된다. \n",
    "\"\"\"\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:94: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:95: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:96: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#========= 게임 환경 로딩 =========#\n",
    "env = gameEnv(partial=False,size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hero (0, 1)\n",
      "goal (3, 3)\n",
      "fire (2, 1)\n",
      "goal (0, 3)\n",
      "fire (4, 4)\n",
      "goal (3, 0)\n",
      "goal (4, 2)\n"
     ]
    }
   ],
   "source": [
    "for item in env.objects:\n",
    "    print(item.name, (item.x, item.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= 신경망 학습 =========#\n",
    "\n",
    "#모든 학습 파라미터를 설정한다            \n",
    "batch_size = 32 #각 학습 단계에서 사용할 경험의 수\n",
    "update_freq = 4 #학습 단계 기준의 업데이트 주기 \n",
    "y = .99 #타겟 Q-값에 대한 할인 계수\n",
    "startE = 1 #시작 시 랜덤 액션의 가능성\n",
    "endE = 0.1 #종료 시 랜덤 액션의 가능성\n",
    "anneling_steps = 10000. #startE에서 endE로 줄어드는데 필요한 학습 단계 수\n",
    "num_episodes = 10000 #네트워크를 학습시키기 위한 게임 환경 에피소드의 수\n",
    "pre_train_steps = 10000 #학습 시작 전 랜덤 액션의 단계 수\n",
    "max_epLength = 50 #허용되는 최대 에피소드 길이\n",
    "load_model = False #저장된 모델을 로딩할 지 여부\n",
    "path = \"./dqn\" #모델을 저장할 경로\n",
    "h_size = 512 #어드밴티지 스트림과 값 스트림으로 분리되기 전의 마지막 컨벌루션 레이어의 크기\n",
    "tau = 0.001 #타겟 네트워크를 제1네트워크로 업데이트시켜 가는 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random_uniform([200, 1,1,512])\n",
    "Y = tf.split(X, 2, 3)\n",
    "Y_shape = tf.shape(Y[1])\n",
    "sess = tf.Session()\n",
    "X_v, Y_v, Y_shape_v = sess.run([X, Y, Y_shape])\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1, 1, 512)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 1, 1, 256), (200, 1, 1, 256))"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_v[0].shape, Y_v[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 224, 224, 3), (5, 224, 224, 3))"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, H, W, C = 5, 224, 224, 3\n",
    "data = np.random.randn(B, H, W, C).astype(np.float32)\n",
    "mask = np.random.randint(2, size=(B, H, W, C)).astype(np.float32)\n",
    "data.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1, 1, 1)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data * mask).sum(axis=(1,2,3), keepdims=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #최종 Q-값을 얻기 위해 어드밴티지 스트림과 값 스트림을 조합해 준다. \n",
    "# Qout = Value + tf.subtract(Advantage,\n",
    "#                            tf.reduce_mean(Advantage, axis=1,\n",
    "#                                           keep_dims=True)\n",
    "#                           )\n",
    "# predict = tf.argmax(Qout, 1)\n",
    "\n",
    "#타겟 Q 값과 예측 Q 값 간의 제곱합 차를 취함으로써 비용을 구한다.\n",
    "targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "actions_onehot = tf.one_hot(actions, 4, # indices : A Tensor of indices\n",
    "                            dtype=tf.float32) # depth : A scalar defining the depth of the one hot dimention\n",
    "\n",
    "# Q = tf.reduce_sum(tf.multiply(Qout, actions_onehot), axis=1)\n",
    "\n",
    "# td_error = tf.square(targetQ - Q)\n",
    "# loss = tf.reduce_mean(td_error)\n",
    "# trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "# updateModel = trainer.minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "t1, t2 = sess.run([actions_onehot, actions], feed_dict={actions:[1,3,4,3,2,2,3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]], dtype=float32), array([1, 3, 4, 3, 2, 2, 3]))"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0.4]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.3 0.  0. ]\n",
      " [0.7 0.  0.  0. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.4, 0. , 0.3, 0.7])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = np.array([.7, .3, .2, .4])\n",
    "onehot_vec = np.array(\n",
    "    [[0,0,0,1],\n",
    "     [0,0,0,0],\n",
    "     [0,1,0,0],\n",
    "     [1,0,0,0]]\n",
    ")\n",
    "print(q1 * onehot_vec)\n",
    "np.sum(q1 * onehot_vec, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저수준의 tensorflow API를 간편하게 사용할 수 있는 고수준 경량 API\n",
    "# https://www.popit.kr/tf-slim-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0/\n",
    "import tensorflow.contrib.slim as slim\n",
    "#네트워크는 게임으로부터 하나의 프레임을 받아 이를 배열로 만든다 (flattening).\n",
    "#배열의 크기를 재조절해주고 4개의 컨벌루션 레이어를 거치면서 처리해 준다.\n",
    "scalarInput =  tf.placeholder(shape=[None,84*84*3],dtype=tf.float32)\n",
    "imageIn = tf.reshape(scalarInput,shape=[-1,84,84,3])\n",
    "conv1 = slim.conv2d(inputs=imageIn, num_outputs=32, kernel_size=[8,8],\n",
    "                    stride=[4,4],padding='VALID', \n",
    "                    biases_initializer=None)\n",
    "conv2 = slim.conv2d(inputs=conv1, num_outputs=64, kernel_size=[4,4],\n",
    "                    stride=[2,2],padding='VALID', \n",
    "                    biases_initializer=None)\n",
    "conv3 = slim.conv2d(inputs=conv2, num_outputs=64, kernel_size=[3,3],\n",
    "                    stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None)\n",
    "conv4 = slim.conv2d(inputs=conv3, num_outputs=h_size, kernel_size=[7,7],\n",
    "                    stride=[1,1],padding='VALID', \n",
    "                    biases_initializer=None)\n",
    "\n",
    "# 마지막 컨벌루션 레이어로부터의 출력값을 취한 후, \n",
    "# 이를 어드밴티지 스트림과 값 스트림으로 분리한다. \n",
    "streamAC, streamVC = tf.split(conv4, 2, 3) # value, num_or_size_splits, axis\n",
    "                                           # 1*1*512 -> [1*1*256, 1*1*256]\n",
    "streamA = slim.flatten(streamAC) # 256\n",
    "streamV = slim.flatten(streamVC) # 256\n",
    "AW = tf.Variable(tf.random_normal([h_size//2, env.actions])) # 256*6\n",
    "VW = tf.Variable(tf.random_normal([h_size//2,1])) # 256*1\n",
    "Advantage = tf.matmul(streamA, AW)\n",
    "Value = tf.matmul(streamV, VW)\n",
    "\n",
    "#최종 Q-값을 얻기 위해 어드밴티지 스트림과 값 스트림을 조합해 준다. \n",
    "Qout = Value + tf.subtract(Advantage,\n",
    "                           tf.reduce_mean(Advantage, axis=1,\n",
    "                                          keep_dims=True)\n",
    "                          )\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "#타겟 Q 값과 예측 Q 값 간의 제곱합 차를 취함으로써 비용을 구한다.\n",
    "targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "actions_onehot = tf.one_hot(actions, env.actions, # indices : A Tensor of indices\n",
    "                            dtype=tf.float32) # depth : A scalar defining the depth of the one hot dimention\n",
    "\n",
    "Q = tf.reduce_sum(tf.multiply(Qout, actions_onehot), axis=1)\n",
    "\n",
    "td_error = tf.square(targetQ - Q)\n",
    "loss = tf.reduce_mean(td_error)\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./tensordata/logs', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= 네트워크 구현 =========#\n",
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #네트워크는 게임으로부터 하나의 프레임을 받아 이를 배열로 만든다 (flattening).\n",
    "        #배열의 크기를 재조절해주고 4개의 컨벌루션 레이어를 거치면서 처리해 준다.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,84*84*3],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #마지막 컨벌루션 레이어로부터의 출력값을 취한 후, 이를 어드밴티지 스트림과 값 스트림으로 분리한다. \n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #최종 Q-값을 얻기 위해 어드밴티지 스트림과 값 스트림을 조합해 준다. \n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #타겟 Q 값과 예측 Q 값 간의 제곱합 차를 취함으로써 비용을 구한다.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "cond : False\n",
      "17\n",
      "cond : True\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "buffer = [5,3,2,2,2,2,1,1,1,3,4,3]; buffer_size = 20\n",
    "print(len(buffer))\n",
    "experience = [5,5,5,5,5]\n",
    "experience1 = [0,1,2,3,6,7,8,1,3,4,5,6,7,1,3]\n",
    "cond = len(buffer) + len(experience) >= buffer_size\n",
    "print('cond :', cond)\n",
    "if cond:\n",
    "    buffer[:(len(experience)+len(buffer)-buffer_size)]\n",
    "buffer.extend(experience)\n",
    "print(len(buffer))\n",
    "cond = len(buffer) + len(experience1) >= buffer_size\n",
    "print('cond :', cond)\n",
    "if cond:\n",
    "    buffer[0:(len(experience1)+len(buffer)-buffer_size)] = []\n",
    "buffer.extend(experience1)\n",
    "print(len(buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 5, 2])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(random.sample([5,3,4,4,4,3,2,2,2], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= 경험 재생 =========#\n",
    "# 다음 클래스는 경험과 샘플을 저장하고 랜덤하게 신경망을 학습시킨다 \n",
    "        \n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(\n",
    "            np.array(\n",
    "                random.sample(self.buffer,\n",
    "                              size)\n",
    "            ),\n",
    "            [size, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#다음은 게임의 프레임의 사이즈를 조절해 주는 간단한 함수이다.\n",
    "def processState(states):\n",
    "    return np.reshape(states,[84*84*3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아래 함수들은 1차 신경망의 파라미터와 함께 목표 신경망의 파라미터를 업데이트하게 해준다\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= 신경망 학습 =========#\n",
    "\n",
    "#모든 학습 파라미터를 설정한다            \n",
    "batch_size = 32 #각 학습 단계에서 사용할 경험의 수\n",
    "update_freq = 4 #학습 단계 기준의 업데이트 주기 \n",
    "y = .99 #타겟 Q-값에 대한 할인 계수\n",
    "startE = 1 #시작 시 랜덤 액션의 가능성\n",
    "endE = 0.1 #종료 시 랜덤 액션의 가능성\n",
    "anneling_steps = 10000. #startE에서 endE로 줄어드는데 필요한 학습 단계 수\n",
    "num_episodes = 10000 #네트워크를 학습시키기 위한 게임 환경 에피소드의 수\n",
    "pre_train_steps = 10000 #학습 시작 전 랜덤 액션의 단계 수\n",
    "max_epLength = 50 #허용되는 최대 에피소드 길이\n",
    "load_model = False #저장된 모델을 로딩할 지 여부\n",
    "path = \"./dqn\" #모델을 저장할 경로\n",
    "h_size = 512 #어드밴티지 스트림과 값 스트림으로 분리되기 전의 마지막 컨벌루션 레이어의 크기\n",
    "tau = 0.001 #타겟 네트워크를 제1네트워크로 업데이트시켜 가는 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables, tau)\n",
    "\n",
    "myBuffer = experience_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Variable 'Conv/weights:0' shape=(8, 8, 3, 32) dtype=float32_ref>,\n",
       "  <tf.Variable 'Conv_1/weights:0' shape=(4, 4, 32, 64) dtype=float32_ref>,\n",
       "  <tf.Variable 'Conv_2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       "  <tf.Variable 'Conv_3/weights:0' shape=(7, 7, 64, 512) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable:0' shape=(256, 4) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_1:0' shape=(256, 1) dtype=float32_ref>],\n",
       " [<tf.Variable 'Conv_4/weights:0' shape=(8, 8, 3, 32) dtype=float32_ref>,\n",
       "  <tf.Variable 'Conv_5/weights:0' shape=(4, 4, 32, 64) dtype=float32_ref>,\n",
       "  <tf.Variable 'Conv_6/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       "  <tf.Variable 'Conv_7/weights:0' shape=(7, 7, 64, 512) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_2:0' shape=(256, 4) dtype=float32_ref>,\n",
       "  <tf.Variable 'Variable_3:0' shape=(256, 1) dtype=float32_ref>])"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainables[:6], trainables[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv/weights/read:0\", shape=(8, 8, 3, 32), dtype=float32)\n",
      "Tensor(\"Conv_1/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\n",
      "Tensor(\"Conv_2/weights/read:0\", shape=(3, 3, 64, 64), dtype=float32)\n",
      "Tensor(\"Conv_3/weights/read:0\", shape=(7, 7, 64, 512), dtype=float32)\n",
      "Tensor(\"Variable/read:0\", shape=(256, 4), dtype=float32)\n",
      "Tensor(\"Variable_1/read:0\", shape=(256, 1), dtype=float32)\n",
      "Tensor(\"Conv_4/weights/read:0\", shape=(8, 8, 3, 32), dtype=float32)\n",
      "Tensor(\"Conv_5/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\n",
      "Tensor(\"Conv_6/weights/read:0\", shape=(3, 3, 64, 64), dtype=float32)\n",
      "Tensor(\"Conv_7/weights/read:0\", shape=(7, 7, 64, 512), dtype=float32)\n",
      "Tensor(\"Variable_2/read:0\", shape=(256, 4), dtype=float32)\n",
      "Tensor(\"Variable_3/read:0\", shape=(256, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for ix, var in enumerate(trainables):\n",
    "    print(var.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 6\n",
      "<tf.Variable 'Conv_4/weights:0' shape=(8, 8, 3, 32) dtype=float32_ref>\n",
      "Tensor(\"add_45:0\", shape=(8, 8, 3, 32), dtype=float32)\n",
      "\n",
      "1 7\n",
      "<tf.Variable 'Conv_5/weights:0' shape=(4, 4, 32, 64) dtype=float32_ref>\n",
      "Tensor(\"add_46:0\", shape=(4, 4, 32, 64), dtype=float32)\n",
      "\n",
      "2 8\n",
      "<tf.Variable 'Conv_6/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
      "Tensor(\"add_47:0\", shape=(3, 3, 64, 64), dtype=float32)\n",
      "\n",
      "3 9\n",
      "<tf.Variable 'Conv_7/weights:0' shape=(7, 7, 64, 512) dtype=float32_ref>\n",
      "Tensor(\"add_48:0\", shape=(7, 7, 64, 512), dtype=float32)\n",
      "\n",
      "4 10\n",
      "<tf.Variable 'Variable_2:0' shape=(256, 4) dtype=float32_ref>\n",
      "Tensor(\"add_49:0\", shape=(256, 4), dtype=float32)\n",
      "\n",
      "5 11\n",
      "<tf.Variable 'Variable_3:0' shape=(256, 1) dtype=float32_ref>\n",
      "Tensor(\"add_50:0\", shape=(256, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for ix, var in enumerate(trainables[0:12//2]):\n",
    "    print()\n",
    "    print(ix, ix+12//2)\n",
    "    print(trainables[ix + 12//2])\n",
    "    print(tau * var.value() + (1-tau)*trainables[ix + 12//2].value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_episodes, max_epLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 % 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#랜덤 액션이 감소하는 비율을 설정 \n",
    "e = startE # 1\n",
    "stepDrop = (startE - endE)/anneling_steps # (1 - 0.1) / 10000.\n",
    "\n",
    "#전체 보상과 에피소드 별 단계 수를 저장할 리스트를 생성\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#모델이 저장될 경로 생성\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    updateTarget(targetOps, sess) #타겟 네트워크가 제1네트워크와 동일하도록 설정\n",
    "    for i in range(num_episodes): # num_episodes\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #환경을 리셋하고 첫번째 관찰 얻기\n",
    "        s = env.reset()\n",
    "        s = processState(s) # 84*84*3=21168\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #Q-네트워크\n",
    "        while j < max_epLength: #만약 에이전트가 블록에 도달하기 위해 200회 이상 시도하면 종료\n",
    "            j+=1\n",
    "            # Q-네트워크로부터 (e의 확률로 랜덤한 액션과 함께) 그리디하게 액션을 선택한다.\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,\n",
    "                             feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            # gameEnv.step()\n",
    "            # penalty = gameEnv.moveChar(action=a)\n",
    "                # 주어진 행동으로 hero를 움직이는 함수\n",
    "                # always 0.\n",
    "            # reward, done = self.checkGoal()\n",
    "                # hero vs others\n",
    "                # hero와 others의 좌표를 비교,\n",
    "                    # 같을 경우 그 객체를 제거하고\n",
    "                    # goal에 갔을 경우 새로운 goal을 생성,\n",
    "                    # fire에 갔을 경우 새로운 fire을 생성\n",
    "                    # 그 reward와 False를 반환\n",
    "                # 같지 않을 경우, return 0.0, False (always)\n",
    "            # state = self.renderEnv()\n",
    "                # 현재 좌표상태를 계산 및 반환\n",
    "                # 84*84*3으로~\n",
    "            s1,r,d = env.step(a) # state, reward, False\n",
    "            s1 = processState(s1) # 84*84*3으로 만들어줌\n",
    "            total_steps += 1\n",
    "            # 에피소드 버퍼에 경험을 저장\n",
    "            # state, action, reward, state2, False\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) \n",
    "            \n",
    "            if total_steps > pre_train_steps: # pre_train_steps\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0: # update_freq = 4\n",
    "                                                     # C step?\n",
    "                    trainBatch = myBuffer.sample(batch_size) # 경험에서 특정 부분을 랜덤하게 획득\n",
    "                                                             # batch_size = 32\n",
    "                                                             # (32, 5)\n",
    "                    #타겟 Q-값에 대해 double DQN 업데이트를 수행\n",
    "                    Q1 = sess.run(mainQN.predict,\n",
    "                                  feed_dict={\n",
    "                                      mainQN.scalarInput:np.vstack(trainBatch[:,3]) # new state\n",
    "                                  })\n",
    "                    Q2 = sess.run(targetQN.Qout,\n",
    "                                  feed_dict={\n",
    "                                      targetQN.scalarInput:np.vstack(trainBatch[:,3]) # new state\n",
    "                                  })\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #타겟 값을 이용해 네트워크를 업데이트\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={\n",
    "                            mainQN.scalarInput : np.vstack(trainBatch[:,0]),\n",
    "                            mainQN.targetQ : targetQ, \n",
    "                            mainQN.actions : trainBatch[:,1]})\n",
    "                    \n",
    "                    updateTarget(targetOps,sess) #타겟 네트워크가 제1네트워크와 동일하도록 설정\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #정기적으로 모델 저장\n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(total_steps,np.mean(rList[-10:]), e)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3],\n",
       "       [ 8],\n",
       "       [13],\n",
       "       [18],\n",
       "       [23],\n",
       "       [28]])"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(np.arange(30).reshape(-1,5)[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= 신경망 학습 확인하기 =========#\n",
    "#시간의 흐름에 따른 평균 보상`\n",
    "\n",
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
