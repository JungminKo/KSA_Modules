{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강사님 : 서울 상명대, 영상처리 pattern recognition 박사님"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 1. 딥러닝 개요**\n",
    "- 딥러닝은 이론적 x, 실험적으로 발전되온 분야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. 딥러닝 개요<br>\n",
    "## 사람에게 쉬운 일이 기계에게는 왜 어려울까?\n",
    "- 다양한 문제들로 인해 분류가 어려울 수 있음\n",
    "    - Viewpoint variation\n",
    "    - illumination conditions\n",
    "    - sclae variation\n",
    "    - deformation\n",
    "    - background color\n",
    "    - occlusion : 가려져있음\n",
    "    - intra-class variation : 예로 가구 제작소에 따라 구분이 힘들 수도..\n",
    "\n",
    "## 인공지능과 머신러닝, 그리고 딥러닝?\n",
    "- AI : 기계에 인간의 지능을 탑재시키는 것\n",
    "- ML : 기계로 학습을 시켜보자 하는 알고리즘 분야\n",
    "- DL : 다양한 ML 알고리즘 중에 신경망 구조 형태를 가지는 것\n",
    "- $DL\\in{ML}\\in{AI}$\n",
    "\n",
    "## 딥러닝, 기존과 무엇이 다른가?\n",
    "- Machine Learning\n",
    "    - input $\\rightarrow$ **feature extraction $\\rightarrow$ classification** $\\rightarrow$ output\n",
    "    - Featurer Extraction : Do Human, Easy Understanding\n",
    "- Deep Learning\n",
    "    - input $\\rightarrow$ **feature extraction + classification** $\\rightarrow$ output\n",
    "    - Featurer Extraction : Do Machine, Hard Understanding\n",
    "\n",
    "## 인공지능의 역사\n",
    "- 1943, Electronic Brain\n",
    "    - Rule Based, 기호주의 인공지능\n",
    "- 1953, Perceptron\n",
    "    - 연결주의 인공지능\n",
    "- 1960~1969, Golden Age\n",
    "    - ADALINE\n",
    "- 1968~1986, XOR Problem\n",
    "- 1995, SVM, Random Forest, etc\n",
    "- 2006, Deep Neutral Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. 인공지능, 머신러닝, 그리고 딥러닝<br>\n",
    "\n",
    "## 인공지능이란?\n",
    "- Artificial Intelligence\n",
    "- Intelligence, 어떻게 정의할까>?\n",
    "\n",
    "## Turing Test\n",
    "- 공간 안에 방이 두 개가 있다. 방 안에는 각각 사람과 기계가 있고 밖에는 또 한 사람이 존재한다.\n",
    "- 방 밖의 사람과 안의 사람과 기계가 서로 채팅을 진행하여, 방 밖의 사람이 방 안에 있는 존재가 사람인지, 기계인지 알 수 있을 것인지를 테스트하는 것\n",
    "\n",
    "## John Searle의 중국어방 논증과 Ray Kurzweil의 반박\n",
    "- 방 안에 중국어를 할 줄 모르는 사람이 존재하고 각각 input과 output에 중국인이 존재한다고 가정하자.\n",
    "- 방 안의 사람은 중국어를 아는 것이 아닌, 메뉴얼을 보고 단순히 번역을 진행하는 것에 불과. 이를 기계가 안다고 얘기할 수 있는가?\n",
    "- Kurzweil은 '특이점이 온다' 서적에서 \"Neuran 하나하나의 관점에서 이들도 그 상황을 정확하게 인지하고 있지 않다.\"라고 반박\n",
    "\n",
    "## 머신러닝이란?\n",
    "- Definition\n",
    "    - Arthur Samuel (1959)\n",
    "        - 명시적으로 프로그램을 작성하지 않고 컴퓨터에 학습할 수 있는 능력을 부여하기 위한 연구 분야\n",
    "    - Tom Mitchell (1997)\n",
    "        - 어떤 **태스크 T**에서 **성능척도 P**로 측정한 성능이 **경험 E**에 따라 향상된다면, 그 컴퓨터 프로그램은 (**태스크 T**와 **성능척도 P**의 관점에서) **경험 E**로부터 학습했다고 말할 수 있다.\n",
    "- 종류\n",
    "    - Supervised Learning\n",
    "        - Classification (분류)\n",
    "        - Regression (회귀)\n",
    "    - Unsupervised Learning\n",
    "        - Clustering (군집)\n",
    "    - Reinforcement Learning\n",
    "\n",
    "## 머신러닝을 하면서 부딪히게 되는 문제\n",
    "- 머신러닝을 하는 목표 : 최적화(optimization)\n",
    "- Underfitting : 최적화를 하지 못한 경우\n",
    "- Overfitting : 훈련 데이터셋에 과적합된 경우\n",
    "    - 모델이 데이터를 외워버리는 것\n",
    "    - 일반화를 못한 경우\n",
    "- Well-Done한 학습기가 되려면 **일반화**가 중요함\n",
    "\n",
    "## 데이터셋 분리하기\n",
    "- Train / Validation / Test\n",
    "- Train : 모델 학습\n",
    "- Validation : hyperparameter 튜닝\n",
    "- Test : 일반화 오류 체크\n",
    "\n",
    "## Artificial Neuron & Aritificial Neural Network\n",
    ">### 들어가기 전에, \n",
    "- 학습시켜놓고 평가가 잘못됬는데 이를 논문을 쓰는 사람들이 많음\n",
    "- 회사에서도 실제로 서비스로 돌아갈 수 있는 모델을 만들어야 하는데 이를 우리가 알아야 함\n",
    "- 코드 돌려서 결과를 보는 것이 의미가 없다고 생각\n",
    "- **데이터에 대한 이해, 과정을 볼 수 있는 통찰력, 결과에 대한 해석이 중요**\n",
    "\n",
    "- 인간의 Neuran을 사장 단순하기 표현한 것이 Perceptron!\n",
    "    - Percept : 인지\n",
    "    - weight : 각각의 데이터에 대한 가중치\n",
    "- 위의 neuron이 여러 층이 쌓인 것을 ANN, DNN이라고 부름\n",
    "    - 각 층을 layer, layer의 한 단위를 unit이라고 부름\n",
    "    - input layer $\\rightarrow$ hidden layer $\\rightarrow$ output layer\n",
    "\n",
    ">### 왜 최근에 와서야?..\n",
    "- 하드웨어 * 데이터 * 알고리즘\n",
    "\n",
    "- ImageNet Challenge\n",
    "    - ImageNet Large Scale Visual Recognition Challenge\n",
    "\n",
    "## 딥러닝의 특징\n",
    "- haha\n",
    "\n",
    "## 딥러닝의 또 다른 표현들\n",
    "- 표현 학습 (Representation Learning)\n",
    "- 종단 간 학습 (End to end Learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@maxtortime_88708/windows-10%EC%97%90%EC%84%9C-tensorflow-gpu-%EC%84%A4%EC%B9%98-5438fea6ed30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://m.blog.naver.com/PostView.nhn?blogId=chrhdhkd&logNo=221082575543&proxyReferer=https%3A%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras가 GPU에서 돌아가고 있는지 확인\n",
    "- https://rfriend.tistory.com/425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from IPython.display import display\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5747712805446674088\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3157432729\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10025497670092546092\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# How to check if the code is running on GPU or CPU?\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to check if Keras is using GPU?\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ndim 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0\n"
     ]
    }
   ],
   "source": [
    "x = np.array(5)\n",
    "print(x, x.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32  5  3 43 27] 1\n"
     ]
    }
   ],
   "source": [
    "x = np.array([32, 5, 3, 43, 27])\n",
    "print(x, x.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32  5  3 43 27]\n",
      " [28 76  0 33 46]\n",
      " [11  9 24 83 47]] 2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[32, 5, 3, 43, 27],\n",
    "              [28, 76, 0, 33, 46],\n",
    "              [11, 9, 24, 83, 47]])\n",
    "print(x, x.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2  3  4  5]\n",
      "  [ 6  7  8  9 10]]\n",
      "\n",
      " [[11 12 13 14 15]\n",
      "  [16 17 18 19 20]]\n",
      "\n",
      " [[21 22 23 24 25]\n",
      "  [26 27 28 29 30]]] 3\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[1, 2, 3, 4, 5], \n",
    "               [6, 7, 8, 9, 10]],\n",
    "              [[11, 12, 13, 14, 15], \n",
    "               [16, 17, 18, 19, 20]],\n",
    "              [[21, 22, 23, 24, 25], \n",
    "               [26, 27, 28, 29, 30]]])\n",
    "print(x, x.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10]],\n",
       "\n",
       "       [[11, 12, 13, 14, 15],\n",
       "        [16, 17, 18, 19, 20]],\n",
       "\n",
       "       [[21, 22, 23, 24, 25],\n",
       "        [26, 27, 28, 29, 30]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 36, 39, 42, 45],\n",
       "       [48, 51, 54, 57, 60]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.product((2,4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]\n",
      "  [ 9 10 11]]\n",
      "\n",
      " [[12 13 14]\n",
      "  [15 16 17]\n",
      "  [18 19 20]\n",
      "  [21 22 23]]] (2, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 4, 3)\n",
    "x = np.arange(np.prod(shape)).reshape(*shape)\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24 27 30 33]\n",
      " [36 39 42 45]] (2, 4)\n"
     ]
    }
   ],
   "source": [
    "axis = 0\n",
    "print(x.sum(axis=axis), x.sum(axis=axis).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  6  8 10]\n",
      " [20 22 24 26]\n",
      " [36 38 40 42]] (3, 4)\n"
     ]
    }
   ],
   "source": [
    "axis = 1\n",
    "print(x.sum(axis=axis), x.sum(axis=axis).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6 22]\n",
      " [38 54]\n",
      " [70 86]] (3, 2)\n"
     ]
    }
   ],
   "source": [
    "axis = 2\n",
    "print(x.sum(axis=axis), x.sum(axis=axis).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.ndim : 0, x.shape : ()\n"
     ]
    }
   ],
   "source": [
    "x = np.array(5)\n",
    "print('x.ndim : {}, x.shape : {}'.format(x.ndim, x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.ndim : 1, x.shape : (5,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([3,5, 10, 44, 103])\n",
    "print('x.ndim : {}, x.shape : {}'.format(x.ndim, x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.ndim : 2, x.shape : (3, 6)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(\n",
    "    [[32, 5, 6, 7, 10, 11],\n",
    "     [26, 76, 0, 33, 46, 1],\n",
    "     [4, 11, 56, 11, 303, 0]]\n",
    ")\n",
    "print('x.ndim : {}, x.shape : {}'.format(x.ndim, x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.ndim : 3, x.shape : (3, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(\n",
    "    [[[20, 1, 55 , 77],\n",
    "      [5, 6, 7, 8],\n",
    "      [111, 14, 254, 0],\n",
    "      [5, 5, 5, 5]],\n",
    "     [[16, 1235, 23, 2],\n",
    "      [1, 3254, 44, 2],\n",
    "      [5, 6, 1, 2],\n",
    "      [0, 0, 0, 0]],\n",
    "     [[1, 1, 1, 1],\n",
    "      [3, 4, 3, 2],\n",
    "      [13, 325, 11, 1412],\n",
    "      [3, 5, 324, 134]]]\n",
    ")\n",
    "print('x.ndim : {}, x.shape : {}'.format(x.ndim, x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[5., 5., 5., 5., 5.],\n",
       "         [5., 5., 5., 5., 5.],\n",
       "         [5., 5., 5., 5., 5.]],\n",
       " \n",
       "        [[5., 5., 5., 5., 5.],\n",
       "         [5., 5., 5., 5., 5.],\n",
       "         [5., 5., 5., 5., 5.]]]), (2, 3, 5))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3, 5)\n",
    "a = np.ones(shape) * 5\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `np.reshape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5., 5., 5., 5., 5.],\n",
       "        [5., 5., 5., 5., 5.],\n",
       "        [5., 5., 5., 5., 5.],\n",
       "        [5., 5., 5., 5., 5.],\n",
       "        [5., 5., 5., 5., 5.],\n",
       "        [5., 5., 5., 5., 5.]]), (6, 5))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.reshape((6, 5))\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims([[1], [2], [3]], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1], [2], [3]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(np.array([[1, 2]]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 32,   5,   6,   7,  10,  11],\n",
       "       [ 26,  76,   0,  33,  46,   1],\n",
       "       [  4,  11,  56,  11, 303,   0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(\n",
    "    [[32, 5, 6, 7, 10, 11],\n",
    "     [26, 76, 0, 33, 46, 1],\n",
    "     [4, 11, 56, 11, 303, 0]]\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 62  92  62  51 359  12] \n",
      "\n",
      "[ 71 182 385] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.sum(axis=0), '\\n')\n",
    "print(x.sum(axis=1), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "# 버전 확인\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 시 불필요한 출력을 끄도록 함 (warning)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 케라스 모델 2가지 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "input_tensor = layers.Input(shape=(784,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 숫자 이미지 분류 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 9s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 1. 데이터 준비\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train[777] : 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "idx = 777\n",
    "print('y_train[{}] : {}'.format(idx, y_train[idx]))\n",
    "plt.imshow(X_train[idx], cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.6791 - acc: 0.8212\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3502 - acc: 0.9021\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3009 - acc: 0.9145\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.2705 - acc: 0.9236\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.2474 - acc: 0.9306\n",
      "[0.23470339568456014, 0.9343333]\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "X_train = X_train.reshape((60000, 28*28)).astype('float32') / 255\n",
    "X_test = X_test.reshape((10000, 28*28)).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# 3. 네트워크 정의\n",
    "model = Sequential()\n",
    "# model.add(Dense(64, input_shape=(28*28,), activation='relu'))\n",
    "model.add(Dense(64, batch_input_shape=(None,28*28,), activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 4. 구성된 네트워크 정보 출력\n",
    "model.summary()\n",
    "\n",
    "# 5. 컴파일\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])\n",
    "\n",
    "# 6. 모델 학습\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "# 7. 모델 평가\n",
    "loss_and_metrics = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 8. 새로운 데이터 예측\n",
    "import numpy as np\n",
    "\n",
    "sample = X_test[0]\n",
    "pred = model.predict_classes(np.expand_dims(sample, axis=0))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 스케일 정규화 안하면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 13.3938 - acc: 0.1689\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 12.9352 - acc: 0.1974\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 12.8400 - acc: 0.2033\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 12.8132 - acc: 0.2050\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 12.9602 - acc: 0.1959\n",
      "[13.292445698038737, 0.17526667]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. 데이터 준비\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "X_train = X_train.reshape((60000, 28*28)).astype('float32')\n",
    "X_test = X_test.reshape((10000, 28*28)).astype('float32')\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# 3. 네트워크 정의\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(28*28,), activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 4. 구성된 네트워크 정보 출력\n",
    "model.summary()\n",
    "\n",
    "# 5. 컴파일\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])\n",
    "\n",
    "# 6. 모델 학습\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "# 7. 모델 평가\n",
    "loss_and_metrics = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 8. 새로운 데이터 예측\n",
    "import numpy as np\n",
    "\n",
    "sample = X_test[0]\n",
    "pred = model.predict_classes(np.expand_dims(sample, axis=0))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate 작게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 8.9498 - acc: 0.4350\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 6.9288 - acc: 0.5648\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 6.1320 - acc: 0.6137\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 4.4399 - acc: 0.7156\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 4.0958 - acc: 0.7390\n",
      "[3.984157050895691, 0.7471167]\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. 데이터 준비\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "X_train = X_train.reshape((60000, 28*28)).astype('float32')\n",
    "X_test = X_test.reshape((10000, 28*28)).astype('float32')\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# 3. 네트워크 정의\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(28*28,), activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 4. 구성된 네트워크 정보 출력\n",
    "model.summary()\n",
    "\n",
    "# 5. 컴파일\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = SGD(lr=0.0001), metrics = ['accuracy'])\n",
    "\n",
    "# 6. 모델 학습\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "# 7. 모델 평가\n",
    "loss_and_metrics = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 8. 새로운 데이터 예측\n",
    "import numpy as np\n",
    "\n",
    "sample = X_test[0]\n",
    "pred = model.predict_classes(np.expand_dims(sample, axis=0))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습한 모델 및 가중치 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.6388 - acc: 0.8359\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3418 - acc: 0.9041\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.2955 - acc: 0.9164\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.2673 - acc: 0.9250\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.2456 - acc: 0.9312\n",
      ">> model saved!!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. 데이터 준비\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "X_train = X_train.reshape((60000, 28*28)).astype('float32') / 255\n",
    "X_test = X_test.reshape((10000, 28*28)).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# 3. 네트워크 정의\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(28*28,), activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 4. 구성된 네트워크 정보 출력\n",
    "model.summary()\n",
    "\n",
    "# 5. 컴파일\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])\n",
    "\n",
    "# 6. 모델 학습\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "### 모델 저장\n",
    "model.save('mnist_mlp_model.h5')\n",
    "print('>> model saved!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습된 모델 및 가중치 불러와서 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> model load!!\n"
     ]
    }
   ],
   "source": [
    "### 모델 불러오기\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('mnist_mlp_model.h5')\n",
    "print('>> model load!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 준비\n",
    "_, (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "X_test = X_test.reshape((10000, 28*28)).astype('float32') / 255\n",
    "\n",
    "# 8. 새로운 데이터 예측\n",
    "sample = X_test[0]\n",
    "pred = model.predict_classes(np.expand_dims(sample, axis=0))\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NAND Gate\n",
    "- 가장 보편적인 논리 회로\n",
    "- Programming에 NAND GATE가 있다면 신경망에는 Perceptron이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 퍼셉트론(perceptron) - 층(layer) 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s 32ms/sample - loss: 0.7087 - acc: 0.5000\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.7077 - acc: 0.2500\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.7068 - acc: 0.2500\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.7059 - acc: 0.2500\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.7051 - acc: 0.2500\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.7044 - acc: 0.2500\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 509us/sample - loss: 0.7037 - acc: 0.2500\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.7030 - acc: 0.2500\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.7025 - acc: 0.2500\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.7019 - acc: 0.2500\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.7014 - acc: 0.2500\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.7009 - acc: 0.2500\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 739us/sample - loss: 0.7005 - acc: 0.2500\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.7001 - acc: 0.2500\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6997 - acc: 0.2500\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6994 - acc: 0.2500\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6990 - acc: 0.2500\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6987 - acc: 0.2500\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6985 - acc: 0.2500\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6982 - acc: 0.2500\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6980 - acc: 0.2500\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 756us/sample - loss: 0.6977 - acc: 0.2500\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6975 - acc: 0.2500\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 757us/sample - loss: 0.6973 - acc: 0.5000\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 996us/sample - loss: 0.6972 - acc: 0.5000\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6970 - acc: 0.5000\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 749us/sample - loss: 0.6968 - acc: 0.5000\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6967 - acc: 0.5000\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6966 - acc: 0.5000\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6964 - acc: 0.5000\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 749us/sample - loss: 0.6963 - acc: 0.5000\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 996us/sample - loss: 0.6962 - acc: 0.5000\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 740us/sample - loss: 0.6961 - acc: 0.5000\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6960 - acc: 0.5000\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6959 - acc: 0.5000\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6958 - acc: 0.5000\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6958 - acc: 0.5000\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6957 - acc: 0.5000\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 750us/sample - loss: 0.6956 - acc: 0.5000\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6956 - acc: 0.5000\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6955 - acc: 0.5000\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6954 - acc: 0.5000\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6954 - acc: 0.5000\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6953 - acc: 0.5000\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6953 - acc: 0.5000\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6952 - acc: 0.5000\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6952 - acc: 0.5000\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6952 - acc: 0.5000\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6951 - acc: 0.5000\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6951 - acc: 0.5000\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6951 - acc: 0.5000\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6950 - acc: 0.5000\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6950 - acc: 0.5000\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6950 - acc: 0.5000\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6949 - acc: 0.5000\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6949 - acc: 0.5000\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6949 - acc: 0.5000\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6948 - acc: 0.5000\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6948 - acc: 0.5000\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6948 - acc: 0.5000\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6948 - acc: 0.5000\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 756us/sample - loss: 0.6948 - acc: 0.5000\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6947 - acc: 0.5000\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6947 - acc: 0.5000\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6947 - acc: 0.5000\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6947 - acc: 0.5000\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6947 - acc: 0.5000\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 749us/sample - loss: 0.6946 - acc: 0.5000\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6946 - acc: 0.5000\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6946 - acc: 0.5000\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6946 - acc: 0.5000\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6946 - acc: 0.5000\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 991us/sample - loss: 0.6946 - acc: 0.5000\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6945 - acc: 0.5000\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 757us/sample - loss: 0.6945 - acc: 0.5000\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 757us/sample - loss: 0.6945 - acc: 0.5000\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6945 - acc: 0.5000\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6945 - acc: 0.5000\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 756us/sample - loss: 0.6945 - acc: 0.5000\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 756us/sample - loss: 0.6945 - acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6944 - acc: 0.5000\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 757us/sample - loss: 0.6944 - acc: 0.5000\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6944 - acc: 0.5000\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6944 - acc: 0.5000\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6944 - acc: 0.5000\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6944 - acc: 0.5000\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6944 - acc: 0.5000\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6944 - acc: 0.5000\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 757us/sample - loss: 0.6943 - acc: 0.5000\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6943 - acc: 0.5000\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6943 - acc: 0.5000\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 988us/sample - loss: 0.6943 - acc: 0.5000\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 992us/sample - loss: 0.6943 - acc: 0.5000\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6943 - acc: 0.5000\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6943 - acc: 0.5000\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6943 - acc: 0.5000\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 988us/sample - loss: 0.6943 - acc: 0.5000\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6942 - acc: 0.5000\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6941 - acc: 0.5000\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6940 - acc: 0.5000\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6938 - acc: 0.5000\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6937 - acc: 0.5000\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6936 - acc: 0.5000\n",
      "Test score: 0.693595290184021\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0., 0.],\n",
    "                   [0., 1.],\n",
    "                   [1., 0.],\n",
    "                   [1., 1.]])\n",
    "y_data = np.array([[0.],\n",
    "                   [1.],\n",
    "                   [1.],\n",
    "                   [0.]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
    "sgd = SGD(lr=0.1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_data, y_data, epochs=200)\n",
    "\n",
    "score = model.evaluate(x_data, y_data, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 다층퍼셉트론(multi-layer perceptron, MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "4/4 [==============================] - 1s 126ms/step - loss: 0.7079 - acc: 0.5000\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 741us/step - loss: 0.7070 - acc: 0.5000\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.7061 - acc: 0.5000\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.7053 - acc: 0.5000\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 489us/step - loss: 0.7045 - acc: 0.5000\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.7039 - acc: 0.5000\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.7032 - acc: 0.7500\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.7026 - acc: 0.7500\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 749us/step - loss: 0.7021 - acc: 0.7500\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.7016 - acc: 0.7500\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.7011 - acc: 0.7500\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 749us/step - loss: 0.7007 - acc: 0.7500\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 747us/step - loss: 0.7003 - acc: 0.7500\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 740us/step - loss: 0.7000 - acc: 0.7500\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6996 - acc: 0.7500\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6993 - acc: 0.7500\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6990 - acc: 0.7500\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6988 - acc: 0.7500\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 489us/step - loss: 0.6985 - acc: 0.7500\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6983 - acc: 0.7500\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6981 - acc: 0.7500\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 743us/step - loss: 0.6979 - acc: 0.7500\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 489us/step - loss: 0.6978 - acc: 0.7500\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6976 - acc: 0.7500\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6974 - acc: 0.7500\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6973 - acc: 0.7500\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6972 - acc: 0.7500\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6970 - acc: 0.7500\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6969 - acc: 0.7500\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6968 - acc: 0.7500\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.6967 - acc: 0.7500\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6966 - acc: 0.7500\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6966 - acc: 0.7500\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6965 - acc: 0.7500\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6964 - acc: 0.7500\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6963 - acc: 0.7500\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6963 - acc: 0.7500\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6962 - acc: 0.7500\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6961 - acc: 0.7500\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6961 - acc: 0.7500\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6960 - acc: 0.7500\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6960 - acc: 0.7500\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6959 - acc: 0.7500\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6959 - acc: 0.7500\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6959 - acc: 0.7500\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6958 - acc: 0.7500\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6958 - acc: 0.7500\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6958 - acc: 0.7500\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6957 - acc: 0.7500\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6957 - acc: 0.7500\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 498us/step - loss: 0.6957 - acc: 0.7500\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6956 - acc: 0.7500\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6956 - acc: 0.7500\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6956 - acc: 0.7500\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6955 - acc: 0.7500\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6955 - acc: 0.7500\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6955 - acc: 0.7500\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6955 - acc: 0.7500\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6955 - acc: 0.7500\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6954 - acc: 0.7500\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6954 - acc: 0.7500\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6954 - acc: 0.7500\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6954 - acc: 0.7500\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6953 - acc: 0.7500\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6953 - acc: 0.7500\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6953 - acc: 0.7500\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6953 - acc: 0.7500\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6953 - acc: 0.7500\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6953 - acc: 0.7500\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6952 - acc: 0.7500\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6952 - acc: 0.7500\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6952 - acc: 0.7500\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 747us/step - loss: 0.6952 - acc: 0.7500\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6952 - acc: 0.7500\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6952 - acc: 0.7500\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6951 - acc: 0.7500\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6951 - acc: 0.7500\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6951 - acc: 0.7500\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6951 - acc: 0.7500\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6951 - acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6951 - acc: 0.7500\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6950 - acc: 0.7500\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6950 - acc: 0.7500\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6950 - acc: 0.7500\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6950 - acc: 0.7500\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6950 - acc: 0.7500\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6950 - acc: 0.7500\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6950 - acc: 0.7500\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 749us/step - loss: 0.6949 - acc: 0.7500\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6949 - acc: 0.7500\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6949 - acc: 0.7500\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6949 - acc: 0.7500\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6949 - acc: 0.7500\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6949 - acc: 0.7500\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6949 - acc: 0.7500\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6948 - acc: 0.7500\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6948 - acc: 0.7500\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6948 - acc: 0.7500\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6948 - acc: 0.7500\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6948 - acc: 0.7500\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6948 - acc: 0.7500\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6948 - acc: 0.7500\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6947 - acc: 0.7500\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6947 - acc: 0.7500\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6947 - acc: 0.7500\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6947 - acc: 0.7500\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6947 - acc: 0.7500\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6947 - acc: 0.7500\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6947 - acc: 0.7500\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6947 - acc: 0.7500\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 747us/step - loss: 0.6946 - acc: 0.7500\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6946 - acc: 0.7500\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6946 - acc: 0.7500\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6946 - acc: 0.7500\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6946 - acc: 0.7500\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6946 - acc: 0.7500\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6946 - acc: 0.7500\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6946 - acc: 0.7500\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6945 - acc: 0.7500\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6945 - acc: 0.7500\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6945 - acc: 0.7500\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6945 - acc: 0.7500\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6945 - acc: 0.7500\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6945 - acc: 0.7500\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6945 - acc: 0.7500\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6945 - acc: 0.7500\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6944 - acc: 0.7500\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6944 - acc: 0.7500\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6944 - acc: 0.7500\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6944 - acc: 0.7500\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 749us/step - loss: 0.6944 - acc: 0.7500\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6944 - acc: 0.7500\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6944 - acc: 0.7500\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6944 - acc: 0.7500\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6943 - acc: 0.7500\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6943 - acc: 0.7500\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6943 - acc: 0.7500\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6943 - acc: 0.7500\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6943 - acc: 0.7500\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6943 - acc: 0.7500\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6943 - acc: 0.7500\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6943 - acc: 0.7500\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.6943 - acc: 0.7500\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6942 - acc: 0.7500\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6942 - acc: 0.7500\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6942 - acc: 0.7500\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6942 - acc: 0.7500\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6942 - acc: 0.7500\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6942 - acc: 0.7500\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6942 - acc: 0.7500\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6942 - acc: 0.7500\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6941 - acc: 0.7500\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6941 - acc: 0.7500\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6941 - acc: 0.7500\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6941 - acc: 0.7500\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6941 - acc: 0.7500\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6941 - acc: 0.7500\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6941 - acc: 0.7500\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6941 - acc: 0.7500\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6941 - acc: 0.7500\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6941 - acc: 0.7500\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6940 - acc: 0.7500\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6940 - acc: 0.7500\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 498us/step - loss: 0.6940 - acc: 0.7500\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6940 - acc: 0.7500\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6940 - acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6940 - acc: 0.7500\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6940 - acc: 0.7500\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6940 - acc: 0.7500\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6940 - acc: 0.7500\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6939 - acc: 0.7500\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6939 - acc: 0.7500\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6939 - acc: 0.7500\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6939 - acc: 0.7500\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6939 - acc: 0.7500\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6939 - acc: 0.7500\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6939 - acc: 0.7500\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6939 - acc: 0.7500\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6939 - acc: 0.7500\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6938 - acc: 0.7500\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6937 - acc: 0.7500\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6937 - acc: 0.7500\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 498us/step - loss: 0.6937 - acc: 0.7500\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6937 - acc: 0.7500\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 749us/step - loss: 0.6937 - acc: 0.7500\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6937 - acc: 0.7500\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6937 - acc: 0.7500\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6937 - acc: 0.7500\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6937 - acc: 0.7500\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6937 - acc: 0.7500\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6936 - acc: 0.5000\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Test score: 0.6936379671096802\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0., 0.],\n",
    "                   [0., 1.],\n",
    "                   [1., 0.],\n",
    "                   [1., 1.]])\n",
    "y_data = np.array([[0.],\n",
    "                   [1.],\n",
    "                   [1.],\n",
    "                   [0.]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "sgd = SGD(lr=0.1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.fit(x_data, y_data, epochs=200)\n",
    "\n",
    "print(model.predict_classes(x_data))\n",
    "\n",
    "score = model.evaluate(x_data, y_data, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR 문제 풀기 직접 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "y\n",
      " [[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(\n",
    "    [[0, 0],\n",
    "     [0, 1],\n",
    "     [1, 0],\n",
    "     [1, 1]]\n",
    ")\n",
    "y = np.array(\n",
    "    [[0],\n",
    "     [1],\n",
    "     [0],\n",
    "     [1]]\n",
    ")\n",
    "print('X\\n', X)\n",
    "print('y\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x1, x2, w, b):\n",
    "    w1, w2 = w\n",
    "    return x1 * w1 + x2 * w2 + b\n",
    "\n",
    "def step_fn(arr, thresh=0):\n",
    "    return np.where(arr > thresh, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 1]), array([0, 0, 0, 1]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = X[:, 0], X[:, 1]\n",
    "w11, b11 = [1, 1], -1.5\n",
    "w12, b12 = [1, 1], -1.5\n",
    "\n",
    "node11 = perceptron(x1, x2, w11, b11)\n",
    "unit11 = step_fn(node11)\n",
    "\n",
    "node12 = perceptron(x1, x2, w12, b12)\n",
    "unit12 = step_fn(node12)\n",
    "\n",
    "unit11, unit12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2, b2 = [1, 1], -1.5\n",
    "node2 = perceptron(unit11, unit12, w2, b2)\n",
    "unit2 = step_fn(node2)\n",
    "unit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18672621])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(4)\n",
    "sum(a / sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units = 5\n",
    "X = units\n",
    "units = 7\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "            [[0, 0],\n",
    "             [0, 1],\n",
    "             [1, 0],\n",
    "             [1, 1]])\n",
    "y = np.array(\n",
    "            [[0],\n",
    "             [1],\n",
    "             [0],\n",
    "             [1]])\n",
    "\n",
    "class Sequential:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        self.weights = defaultdict(dict)\n",
    "#         self.errors = defaultdict(dict)\n",
    "        \n",
    "    def add(self, dense):\n",
    "        self.models.append(dense)\n",
    "        for i in range(dense.units):\n",
    "            rand_weight = np.random.rand(dense.input_dim)\n",
    "            self.weights[dense][i] = {\n",
    "                'w' : rand_weight / sum(rand_weight), # 가중치의 합을 1로\n",
    "                'b' : np.random.rand(1)\n",
    "            }\n",
    "    \n",
    "    def compiled(self, loss, optimizer, metrics):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000):\n",
    "        X = self.X.copy()\n",
    "        # 초기에 설정한 epoch 수만큼 loop\n",
    "        for epoch in epochs:\n",
    "            # 설정한 layer 수만큼 loop\n",
    "            for dense in self.models:\n",
    "                # 다음 레이어를 위한 input data 준비\n",
    "                units = np.array([]).reshape(dense.input_dim, 0)\n",
    "                # layer별 node 수만큼 loop\n",
    "                for i in self.weights[dense].keys():\n",
    "                    W = self.weights[dense][i]['w']\n",
    "                    b = self.weights[dense][i]['b']\n",
    "                    node = dense.perceptron(X, W, b)\n",
    "                    if dense.activation.upper() == 'SIGMOID':\n",
    "                        unit = dense.sigmoid(node)\n",
    "                    elif dense.activation.upper() == 'STEP':\n",
    "                        unit = dense.step(node)\n",
    "                    units = np.hstack((units, unit.reshape(-1, 1)))\n",
    "                X = units\n",
    "            # 해당 epoch 종료 후 error 계산\n",
    "            \n",
    "                    \n",
    "                        \n",
    "    \n",
    "    def predict_classes(self, test):\n",
    "        pass\n",
    "    \n",
    "    def summmary(self, ):\n",
    "        pass\n",
    "    \n",
    "    def evalueate(self, ):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class Dense:\n",
    "    \n",
    "    def __init__(self, units, input_dim, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        actiation = 'sigmoid' or 'step'\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation\n",
    "\n",
    "    def perceptron(self, X, W, b):\n",
    "        return X.dot(W) + b\n",
    "        \n",
    "    def step(self, arr, thresh=0):\n",
    "        return np.where(arr > thersh, 1, 0)\n",
    "    \n",
    "    def sigmoid(self, arr):\n",
    "        return 1 / (1 + np.exp(-arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [2., 2.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2, activation='sigmoid'))\n",
    "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
    "# model.compile()\n",
    "model.fit(X, y, epochs=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
